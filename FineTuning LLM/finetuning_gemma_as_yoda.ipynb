{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"a958b107d35b4391a3c617411c1e08d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01e1a962fa0044588db878a1272b6b8e","IPY_MODEL_d5ecbe9b01d645c89af417f0363b17a5","IPY_MODEL_846821d7ae1c4a9c8e2a20d79b0dbaed"],"layout":"IPY_MODEL_61a80865d404443d9e2f2e0658276a82"}},"01e1a962fa0044588db878a1272b6b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e9236e5161943318b9cd0f34fa13c79","placeholder":"​","style":"IPY_MODEL_906ba0b9a99f4a0cb742d014ae06dc22","value":"tokenizer_config.json: 100%"}},"d5ecbe9b01d645c89af417f0363b17a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51fabca5c36c4989a6d218032782418c","max":47022,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9374b7e038047abb08feefbff9d099e","value":47022}},"846821d7ae1c4a9c8e2a20d79b0dbaed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d48bc39906ef4bb3afc0668e4fde67be","placeholder":"​","style":"IPY_MODEL_ac02ccb43d1b4de7988eee1e24dde832","value":" 47.0k/47.0k [00:00&lt;00:00, 1.40MB/s]"}},"61a80865d404443d9e2f2e0658276a82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e9236e5161943318b9cd0f34fa13c79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"906ba0b9a99f4a0cb742d014ae06dc22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51fabca5c36c4989a6d218032782418c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9374b7e038047abb08feefbff9d099e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d48bc39906ef4bb3afc0668e4fde67be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac02ccb43d1b4de7988eee1e24dde832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7afb19b437c14cd186ba5d4281cb400c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf31b46e4f184974819571708be8844c","IPY_MODEL_c8ca6435887a42e28354e53a290a0aaa","IPY_MODEL_64feafd311d64770947a14a11ef27275"],"layout":"IPY_MODEL_adec01e7056641e9b752414bf91ecc0d"}},"cf31b46e4f184974819571708be8844c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80ed4b19fe87450b819c243ee15b850a","placeholder":"​","style":"IPY_MODEL_f529270ca437478980d2d00fedaf4d42","value":"tokenizer.model: 100%"}},"c8ca6435887a42e28354e53a290a0aaa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10952dd8306c428e9be8e17345b557b5","max":4241003,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b51a6066927c4a6cb19a59bd0db9442f","value":4241003}},"64feafd311d64770947a14a11ef27275":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46467ac7800d46a08fc52cc13d9d6357","placeholder":"​","style":"IPY_MODEL_453106d1cacc4e9dae7b2aeda770fc2f","value":" 4.24M/4.24M [00:00&lt;00:00, 19.6MB/s]"}},"adec01e7056641e9b752414bf91ecc0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80ed4b19fe87450b819c243ee15b850a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f529270ca437478980d2d00fedaf4d42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10952dd8306c428e9be8e17345b557b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b51a6066927c4a6cb19a59bd0db9442f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46467ac7800d46a08fc52cc13d9d6357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"453106d1cacc4e9dae7b2aeda770fc2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2349949170414be989ffdb9c2fb7fc05":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c42b8298a58428f8a09467c9875e9c7","IPY_MODEL_5d85bbd8d4c34996a7216078bcb06e65","IPY_MODEL_f4b85921603341978cf8f7614638a9fd"],"layout":"IPY_MODEL_48f3aa349b3d402dbedb4f67da900231"}},"7c42b8298a58428f8a09467c9875e9c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c1fdad0bbe742e6904a565708e0199c","placeholder":"​","style":"IPY_MODEL_681b417af4e44482a7d9c1702d131a48","value":"tokenizer.json: 100%"}},"5d85bbd8d4c34996a7216078bcb06e65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb9bb92d00f04ec8b7a4dc2692fe10fc","max":17525357,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a11d690f12304b1696be11c831eb52d9","value":17525357}},"f4b85921603341978cf8f7614638a9fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37d26f981b9b40d3a18f10c7b70e7641","placeholder":"​","style":"IPY_MODEL_c47d96756e4e458ab6e380a90a358e08","value":" 17.5M/17.5M [00:00&lt;00:00, 37.4MB/s]"}},"48f3aa349b3d402dbedb4f67da900231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c1fdad0bbe742e6904a565708e0199c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"681b417af4e44482a7d9c1702d131a48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb9bb92d00f04ec8b7a4dc2692fe10fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a11d690f12304b1696be11c831eb52d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37d26f981b9b40d3a18f10c7b70e7641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c47d96756e4e458ab6e380a90a358e08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de734994a73b4eb18dd3b0ff7b74f0ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e79b7ed4268744e1bf4eec88828b7d24","IPY_MODEL_27cdf4cab0bc4af9aaee6112e6625ed5","IPY_MODEL_e23dfd13d3e44226b556ed7e9b339705"],"layout":"IPY_MODEL_b3be29bd3cde4832bc93123c33d18da8"}},"e79b7ed4268744e1bf4eec88828b7d24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32e498c61a1c4a90bdd0f1f6bdd87698","placeholder":"​","style":"IPY_MODEL_7723c5bdbc1048f9be2ef81722d9ba46","value":"special_tokens_map.json: 100%"}},"27cdf4cab0bc4af9aaee6112e6625ed5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00cb9d298e2644b29cafd6d5384df61f","max":636,"min":0,"orientation":"horizontal","style":"IPY_MODEL_535cc19c55134210a3412d68f1ba3471","value":636}},"e23dfd13d3e44226b556ed7e9b339705":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3094e8876de4995914482e4711f5c6d","placeholder":"​","style":"IPY_MODEL_57e8463d425e468f876dd0f8af4dfbd3","value":" 636/636 [00:00&lt;00:00, 15.3kB/s]"}},"b3be29bd3cde4832bc93123c33d18da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e498c61a1c4a90bdd0f1f6bdd87698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7723c5bdbc1048f9be2ef81722d9ba46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00cb9d298e2644b29cafd6d5384df61f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535cc19c55134210a3412d68f1ba3471":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3094e8876de4995914482e4711f5c6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57e8463d425e468f876dd0f8af4dfbd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7811a9fb6d64427b79d7b57ba7fc551":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dda584953fe548cd96deaad4ea4c7ec1","IPY_MODEL_5fa3e82413dc46129b2b88503603135f","IPY_MODEL_b551fc37b63c49c5b638ffaa1915491b"],"layout":"IPY_MODEL_49e19e1864be4c7e91c9a682fb34f669"}},"dda584953fe548cd96deaad4ea4c7ec1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2156a28dc8184116b2bbd51acb6a780f","placeholder":"​","style":"IPY_MODEL_c108acb4d6654ad682af04e13eafbc7a","value":"config.json: 100%"}},"5fa3e82413dc46129b2b88503603135f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23b11147585043a5a157fd1a2b66ecd4","max":913,"min":0,"orientation":"horizontal","style":"IPY_MODEL_821e665f9d0a41cf9fe4feee51c61aa4","value":913}},"b551fc37b63c49c5b638ffaa1915491b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fee0c2b00b140c396df817896bd2fb5","placeholder":"​","style":"IPY_MODEL_edfc60765cbe4df6b341e0d675d24ce7","value":" 913/913 [00:00&lt;00:00, 32.9kB/s]"}},"49e19e1864be4c7e91c9a682fb34f669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2156a28dc8184116b2bbd51acb6a780f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c108acb4d6654ad682af04e13eafbc7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23b11147585043a5a157fd1a2b66ecd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"821e665f9d0a41cf9fe4feee51c61aa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fee0c2b00b140c396df817896bd2fb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edfc60765cbe4df6b341e0d675d24ce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dbc03ef548c4945b40a07769ef55007":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74a1dcd97f9940d7ab4e836c2002f6ac","IPY_MODEL_6cb31cccdf5948abb0fd476f0cda6d2e","IPY_MODEL_0e794032521a455f8f86ad152c7759bc"],"layout":"IPY_MODEL_a4a5d0d0ff0345bdaf41eeac3bf7b7e6"}},"74a1dcd97f9940d7ab4e836c2002f6ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a6cf71c67a1442d942385d2feb63adb","placeholder":"​","style":"IPY_MODEL_80007d6eb49a4b9493680462c2f2a2a4","value":"model.safetensors: 100%"}},"6cb31cccdf5948abb0fd476f0cda6d2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_344da945ef4349899f6d2ab3d21d9f95","max":5228717512,"min":0,"orientation":"horizontal","style":"IPY_MODEL_897ada49ac6d424589738f86c86dff06","value":5228717512}},"0e794032521a455f8f86ad152c7759bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cb3769857d9444a9dac7f4d6ed5a0aa","placeholder":"​","style":"IPY_MODEL_006afbe2f3ef455d9a377945b2dd3625","value":" 5.23G/5.23G [02:03&lt;00:00, 41.7MB/s]"}},"a4a5d0d0ff0345bdaf41eeac3bf7b7e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a6cf71c67a1442d942385d2feb63adb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80007d6eb49a4b9493680462c2f2a2a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"344da945ef4349899f6d2ab3d21d9f95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"897ada49ac6d424589738f86c86dff06":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3cb3769857d9444a9dac7f4d6ed5a0aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"006afbe2f3ef455d9a377945b2dd3625":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a045e66dba74a6ea198abbe408195ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbb663df459b41899bb020dbd1e112e9","IPY_MODEL_0b2aea5b9e434053ae77356211e57b0e","IPY_MODEL_2a36d08200634e758d946c7081688f65"],"layout":"IPY_MODEL_7f24c959350a4fb891dae777061e6bc5"}},"fbb663df459b41899bb020dbd1e112e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d31acd86a16a4fc49bb87f6aac52fd53","placeholder":"​","style":"IPY_MODEL_8f0b3975a86a4abbb05de543854e1bc9","value":"generation_config.json: 100%"}},"0b2aea5b9e434053ae77356211e57b0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d52d606a4ec403eadd200e5050a654d","max":209,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1392ddeb7d5e42d1b366d9d7267bb214","value":209}},"2a36d08200634e758d946c7081688f65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64c307766b094fdd832903efd26c5c67","placeholder":"​","style":"IPY_MODEL_ed8b1004a22e406dae96344577f6c66a","value":" 209/209 [00:00&lt;00:00, 14.7kB/s]"}},"7f24c959350a4fb891dae777061e6bc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d31acd86a16a4fc49bb87f6aac52fd53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f0b3975a86a4abbb05de543854e1bc9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d52d606a4ec403eadd200e5050a654d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1392ddeb7d5e42d1b366d9d7267bb214":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64c307766b094fdd832903efd26c5c67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed8b1004a22e406dae96344577f6c66a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31e98d83662e479d98996e482b42375a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ad0c979868c49ee88045021f05588db","IPY_MODEL_41111da607a34da5bca0f40ecf906064","IPY_MODEL_446dfc21c8ec4de5a1cebcc045887d94"],"layout":"IPY_MODEL_62300c444df5409289e04842c553ebf5"}},"0ad0c979868c49ee88045021f05588db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31653d7363f34196b05f5834e21e4b63","placeholder":"​","style":"IPY_MODEL_2eba23e1f40c468a91d4c938d6d521a6","value":"README.md: 100%"}},"41111da607a34da5bca0f40ecf906064":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b52727f090648b2bd0724b593248a85","max":8199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_778251869329449ea8a0d647ec0af0a0","value":8199}},"446dfc21c8ec4de5a1cebcc045887d94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eb24d6f2cf84a82bd0750842dc58417","placeholder":"​","style":"IPY_MODEL_d3bb17d46feb4c5197e601809ccdc1a2","value":" 8.20k/8.20k [00:00&lt;00:00, 589kB/s]"}},"62300c444df5409289e04842c553ebf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31653d7363f34196b05f5834e21e4b63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eba23e1f40c468a91d4c938d6d521a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b52727f090648b2bd0724b593248a85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"778251869329449ea8a0d647ec0af0a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2eb24d6f2cf84a82bd0750842dc58417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3bb17d46feb4c5197e601809ccdc1a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5cef58a6bb04aa091711e22d3ea9d64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ca98ea44aed4a2fb53b356d198ad58c","IPY_MODEL_681fc5d6d34d41ca85ae17df0275a3f9","IPY_MODEL_b8951fa4d2c8474ba354f9e4856096b9"],"layout":"IPY_MODEL_47439fb4ec124eb78383dc678391eb8e"}},"3ca98ea44aed4a2fb53b356d198ad58c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66a2f747e6514bac866bd4ddb4c08e25","placeholder":"​","style":"IPY_MODEL_3668966ea3334362885bdee485fd3b4d","value":"databricks-dolly-15k.jsonl: 100%"}},"681fc5d6d34d41ca85ae17df0275a3f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dff208a26c349f989821c9c2223f5d6","max":13085339,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f73df80685974a2998218cdf08cf3da1","value":13085339}},"b8951fa4d2c8474ba354f9e4856096b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b8a71492651458a9256108ff0252c6c","placeholder":"​","style":"IPY_MODEL_37bbfa436fb443c79d5e051d8f9148d7","value":" 13.1M/13.1M [00:00&lt;00:00, 43.6MB/s]"}},"47439fb4ec124eb78383dc678391eb8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66a2f747e6514bac866bd4ddb4c08e25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3668966ea3334362885bdee485fd3b4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dff208a26c349f989821c9c2223f5d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73df80685974a2998218cdf08cf3da1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b8a71492651458a9256108ff0252c6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37bbfa436fb443c79d5e051d8f9148d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4632452ad2064693afb9ba0cdc0b79f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8029bbabbf6a48a9954790554c502ffe","IPY_MODEL_1f7d20fb53884861a71fb9e4922ccb22","IPY_MODEL_c569c5aaf3874657812f5b5bc2ee5f38"],"layout":"IPY_MODEL_0d7c9ec329f24142b72795caf3494458"}},"8029bbabbf6a48a9954790554c502ffe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd7ec4cf69dc416c9d30da5688ca4843","placeholder":"​","style":"IPY_MODEL_92873ca8586246e9aada5e70b6de6917","value":"Generating train split: 100%"}},"1f7d20fb53884861a71fb9e4922ccb22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d9a9a7117934c87b3dbc769127da989","max":15011,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a10e4a3ad8a4290ba2c1737c279758e","value":15011}},"c569c5aaf3874657812f5b5bc2ee5f38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b169ab2ebae45719c8be5898fccee0f","placeholder":"​","style":"IPY_MODEL_399b2e7dbd904cceb32ba6645a4a208f","value":" 15011/15011 [00:00&lt;00:00, 67860.76 examples/s]"}},"0d7c9ec329f24142b72795caf3494458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd7ec4cf69dc416c9d30da5688ca4843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92873ca8586246e9aada5e70b6de6917":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d9a9a7117934c87b3dbc769127da989":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a10e4a3ad8a4290ba2c1737c279758e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b169ab2ebae45719c8be5898fccee0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"399b2e7dbd904cceb32ba6645a4a208f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a69b90f7930b4f2db93982fe99cc21b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92fe165cc8514953b151b809ab3101e0","IPY_MODEL_face7f25b521428b8b9022ad38b1d617","IPY_MODEL_01f4629eb6d1458f9764e19fce216775"],"layout":"IPY_MODEL_a206e7b3ef8148a184fe8eec76b5c4ba"}},"92fe165cc8514953b151b809ab3101e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19c106c16a28414487437fdbd1b51c93","placeholder":"​","style":"IPY_MODEL_1b4fd2ddbff34efa98b8aa2d21f00ac3","value":"Map: 100%"}},"face7f25b521428b8b9022ad38b1d617":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef7750a1cf0b43ceb9a28355b02e2f5c","max":2048,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43c3e242d92b489db9bf5f4c3d9cecdc","value":2048}},"01f4629eb6d1458f9764e19fce216775":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24a539b9bda742cca5e767fe89a91bc5","placeholder":"​","style":"IPY_MODEL_d1d9e5b0448047889c9abd62b2fceac1","value":" 2048/2048 [00:00&lt;00:00, 15242.41 examples/s]"}},"a206e7b3ef8148a184fe8eec76b5c4ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19c106c16a28414487437fdbd1b51c93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b4fd2ddbff34efa98b8aa2d21f00ac3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef7750a1cf0b43ceb9a28355b02e2f5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43c3e242d92b489db9bf5f4c3d9cecdc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24a539b9bda742cca5e767fe89a91bc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d9e5b0448047889c9abd62b2fceac1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9164685f4264462289c0f4d501bde3f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c995cccd28084ffb9c85d0b9a5ae0654","IPY_MODEL_5adb9f0f5c0e404f92ee5c84068b7df6","IPY_MODEL_a43c6a271d2e486ba55d99fefbdd3c41"],"layout":"IPY_MODEL_29d0084620b849f4b459cc7bd777ad36"}},"c995cccd28084ffb9c85d0b9a5ae0654":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d06fe39dc804bdea5753ff675a7df9a","placeholder":"​","style":"IPY_MODEL_0efe1892a26849c0a4876e81442a675f","value":"Map: 100%"}},"5adb9f0f5c0e404f92ee5c84068b7df6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98a48d7485e44e75857a5ca3c5a7c1da","max":2048,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d4b7f868d8141d8aaa84794666beca0","value":2048}},"a43c6a271d2e486ba55d99fefbdd3c41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dbc0c6acfd24332b36c11effe5c55d0","placeholder":"​","style":"IPY_MODEL_5ea514963f2e44e4b1da90ae119303e8","value":" 2048/2048 [00:00&lt;00:00, 12801.68 examples/s]"}},"29d0084620b849f4b459cc7bd777ad36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d06fe39dc804bdea5753ff675a7df9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0efe1892a26849c0a4876e81442a675f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98a48d7485e44e75857a5ca3c5a7c1da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d4b7f868d8141d8aaa84794666beca0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dbc0c6acfd24332b36c11effe5c55d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ea514963f2e44e4b1da90ae119303e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10787717,"sourceType":"datasetVersion","datasetId":6694536}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"*****************\n*****************\n# Large Language Model (LLM) Fine-tuning 🚀🤖\n\n## Introduction\n\nIn this lab, I will explore **fine-tuning large language models (LLMs)** to generate responses in a specific style. Instead of training a model from scratch, I will take an existing pre-trained model and adapt it to a particular dataset using **low-rank adaptation (LoRA)**, an efficient fine-tuning technique.  🎯\n\nThis process will help me understand:\n✅ How tokenization works in LLMs. 🏷️\n\n✅ The importance of fine-tuning and how it modifies a model’s behavior. 🔄\n\n✅ How to use LoRA for efficient training instead of modifying all model weights. ⚡\n\n✅ How to evaluate the fine-tuned model’s performance using external tools. 📊\n\n\nBy the end of this lab, I aim to build a more personalized and efficient LLM that aligns with a specific task or persona! 🚀🔥\n---\n---","metadata":{"id":"K_lm0FmJwiD-"}},{"cell_type":"markdown","source":"## 🛠️ Setting Up the Environment 🚀  \n\nBefore starting, I need to **install and import** the required libraries. These libraries will help in **data processing, model training, and evaluation**. 📊🔍  \n\nI will be using **Hugging Face**, a platform that provides access to various LLMs and datasets. 🏛️  \n\n### 🔹 Tools & Technologies:  \n✅ A **pre-trained LLM** (*Google's Gemma 2B*) as the base model. 🤖  \n✅ A **Hugging Face dataset** for training. 📚  \n✅ **LoRA (Low-Rank Adaptation)**, which updates only a small subset of model parameters, making fine-tuning more efficient. ⚡  \n✅ **Evaluation tools** to measure the performance of my fine-tuned model. 📈  \n\n---\n","metadata":{"id":"6kv9fFjXdl6h"}},{"cell_type":"code","source":"import warnings\n\n# Ignore deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:49:10.027774Z","iopub.execute_input":"2025-02-18T21:49:10.028120Z","iopub.status.idle":"2025-02-18T21:49:10.032464Z","shell.execute_reply.started":"2025-02-18T21:49:10.028090Z","shell.execute_reply":"2025-02-18T21:49:10.031459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install and import MIT Deep Learning utilities\n!pip install mitdeeplearning > /dev/null 2>&1 -q\nimport mitdeeplearning as mdl","metadata":{"id":"fmkjWI4fVeAh","executionInfo":{"status":"ok","timestamp":1739338849714,"user_tz":-300,"elapsed":119421,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:20:28.348794Z","iopub.execute_input":"2025-02-18T21:20:28.349118Z","iopub.status.idle":"2025-02-18T21:20:35.370785Z","shell.execute_reply.started":"2025-02-18T21:20:28.349092Z","shell.execute_reply":"2025-02-18T21:20:35.369224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom lion_pytorch import Lion","metadata":{"id":"Oo64stjwBvnB","executionInfo":{"status":"ok","timestamp":1739338862947,"user_tz":-300,"elapsed":13241,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:10.453241Z","iopub.execute_input":"2025-02-18T20:54:10.453918Z","iopub.status.idle":"2025-02-18T20:54:15.160838Z","shell.execute_reply.started":"2025-02-18T20:54:10.453884Z","shell.execute_reply":"2025-02-18T20:54:15.160177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🎯 Understanding Fine-tuning an LLM for Style ✨  \n\nFine-tuning is a technique where I take a **pre-trained model** and train it further on a **specific dataset** to make it perform better in a specialized task. 🏋️‍♂️📚  \n\nInstead of learning from scratch, the model **adapts its existing knowledge** to new data. 🔄  \n\n### 📝 What Will Happen?  \n✅ The LLM will be **fine-tuned** to generate text in a particular style. 🎭  \n✅ The model will **learn patterns** from the dataset and generate responses that match that style. 🗣️  \n✅ Since fine-tuning an LLM requires **a lot of computational power** ⚡💻, I will use **LoRA (Low-Rank Adaptation)** to make the process more efficient. 🚀  \n\n---\n","metadata":{"id":"6Ao0ECa1wiEA"}},{"cell_type":"markdown","source":"## 📝 Question-Answer Template 🤖💬  \n\nWe define a **structured format** for user queries and AI responses to ensure clear and effective communication. 🏗️📌  \n\n### 📌 Template Breakdown  \n🔹 **Template Without Answer**:  \n   - Uses placeholders for user questions, keeping the conversation structured. ❓  \n🔹 **Template With Answer**:  \n   - Extends the format by including AI-generated responses for **consistency and clarity**. 💡  \n🔹 **Testing the Template**:  \n   - By inserting an example, we **visualize structured interactions** to improve AI training. 🧠📊  \n\n✅ This **standardized approach** helps in organizing conversational data, making AI learning **more efficient**! 🚀🤓  \n","metadata":{"id":"Ma-rp-LbwiEA"}},{"cell_type":"code","source":"# Basic question-answer template\ntemplate_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\ntemplate_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n\n# Let's try to put something into the template to see how it looks\nprint(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Gemma!\"))","metadata":{"id":"TN2zHVhfBvnE","outputId":"ec533022-9eb6-4a62-afa5-cb74920be976","executionInfo":{"status":"ok","timestamp":1739338862948,"user_tz":-300,"elapsed":9,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:15.161998Z","iopub.execute_input":"2025-02-18T20:54:15.162618Z","iopub.status.idle":"2025-02-18T20:54:15.167094Z","shell.execute_reply.started":"2025-02-18T20:54:15.162592Z","shell.execute_reply":"2025-02-18T20:54:15.166244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🔡 Loading the Tokenizer for Gemma 2B 🏗️📖  \n\n🔹 **Model Selection**:  \n   - We specify the model ID (`unsloth/gemma-2-2b-it`) to load the tokenizer. 🎯  \n\n🔹 **Initializing the Tokenizer**:  \n   - The `AutoTokenizer` automatically retrieves the correct tokenizer based on the model. 🔄🔍  \n\n🔹 **Vocabulary Size Check**:  \n   - We print the total number of **unique tokens** the model can recognize. 🧠📊  \n   - This helps in understanding the model's **linguistic range** and processing power. 📈💬  \n\n✅ **Why This Matters?**  \nProper tokenization ensures **smooth text processing** before feeding data into the model, leading to **better fine-tuning results**! 🚀🤖  \n","metadata":{"id":"WknJfayjhs0_"}},{"cell_type":"code","source":"# Load the tokenizer for Gemma 2B\nmodel_id = \"unsloth/gemma-2-2b-it\" #\"google/gemma-2-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# How big is the tokenizer?\nprint(f\"Vocab size: {len(tokenizer.get_vocab())}\")","metadata":{"id":"EeDF1JI-BvnF","outputId":"9c21263d-e259-4482-cfb4-721f6bab07aa","executionInfo":{"status":"ok","timestamp":1739338870537,"user_tz":-300,"elapsed":7596,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:26.959155Z","iopub.execute_input":"2025-02-18T20:54:26.959456Z","iopub.status.idle":"2025-02-18T20:54:30.536504Z","shell.execute_reply.started":"2025-02-18T20:54:26.959433Z","shell.execute_reply":"2025-02-18T20:54:30.535623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🛠️ Testing Tokenization and Decoding 🔡🔍  \n\n🔹 **Input Text**:  \n   - We define a **sample sentence** to test the tokenizer. ✍️💬  \n\n🔹 **Tokenization**:  \n   - The `encode` function **converts text** into numerical **token IDs**, which the model understands. 🔢➡️🤖  \n\n🔹 **Decoding**:  \n   - The `decode` function **transforms token IDs back** into **human-readable text**, ensuring accuracy. 🔄📖  \n\n✅ **Why This Matters?**  \nThis step ensures that the tokenizer correctly processes text, maintaining **meaning and coherence**, which is essential for high-quality AI responses! 🚀🧠  \n","metadata":{"id":"v9-M7L_OwiEC"}},{"cell_type":"code","source":"# Lets test out both steps:\ntext = \"Here is some sample text!\"\nprint(f\"Original text: {text}\")\n\n# Tokenize the text\ntokens = tokenizer.encode(text, return_tensors=\"pt\")\nprint(f\"Encoded tokens: {tokens}\")\n\n# Decode the tokens\ndecoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\nprint(f\"Decoded text: {decoded_text}\")","metadata":{"id":"JH1XzPkiBvnF","outputId":"19f934bd-311d-4664-aab6-ca2ae3f6597d","executionInfo":{"status":"ok","timestamp":1739338870538,"user_tz":-300,"elapsed":17,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:38.252668Z","iopub.execute_input":"2025-02-18T20:54:38.253018Z","iopub.status.idle":"2025-02-18T20:54:38.281251Z","shell.execute_reply.started":"2025-02-18T20:54:38.252989Z","shell.execute_reply":"2025-02-18T20:54:38.280313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Formatting a Prompt  \n\n- **Using the Template**: We format the `template_without_answer` with a specific question: *\"What is the capital of France? Use one word.\"*  \n- **Purpose**: This helps structure user input in a way that aligns with the expected model response format.  \n- **Output**: The formatted prompt follows the predefined structure but does not yet include an answer.  \n\nThis step ensures consistency when interacting with the model.  \n","metadata":{"id":"55Q4YNknwiED"}},{"cell_type":"code","source":"prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\nprint(prompt)","metadata":{"id":"jyBxl6NIBvnF","outputId":"01f9c8a5-6ce9-4ca9-a6d6-9370d4ff0183","executionInfo":{"status":"ok","timestamp":1739338870538,"user_tz":-300,"elapsed":15,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:43.551885Z","iopub.execute_input":"2025-02-18T20:54:43.552166Z","iopub.status.idle":"2025-02-18T20:54:43.556519Z","shell.execute_reply.started":"2025-02-18T20:54:43.552143Z","shell.execute_reply":"2025-02-18T20:54:43.555376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we were to feed this to the model, it would see that it is now the start of the model's turn, and it would generate the answer to this question.","metadata":{"id":"bpRVKdq2wiED"}},{"cell_type":"markdown","source":"----\n### Loading the Model  \n\n- **Purpose**: This step loads the pre-trained language model.  \n- **Process**:  \n  - `AutoModelForCausalLM.from_pretrained(model_id)`: Loads the model weights and architecture based on the `model_id`.  \n  - `device_map=\"auto\"`: Automatically assigns the model to the best available hardware (e.g., GPU if available).  \n- **Time Consideration**: Loading a large model may take a few minutes, depending on hardware capabilities.  \n\nThis step is essential before generating responses using the model.\n\n------\n","metadata":{"id":"okofCJczwiED"}},{"cell_type":"code","source":"# Load the model -- note that this may take a few minutes\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")","metadata":{"id":"mWtWvgiuBvnG","outputId":"fa4b1358-bac3-4cf1-c2b2-288ba8b59f2e","executionInfo":{"status":"ok","timestamp":1739339022868,"user_tz":-300,"elapsed":152343,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:48.548843Z","iopub.execute_input":"2025-02-18T20:54:48.549148Z","iopub.status.idle":"2025-02-18T20:57:01.931348Z","shell.execute_reply.started":"2025-02-18T20:54:48.549120Z","shell.execute_reply":"2025-02-18T20:57:01.930735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🤖 Generating a Response from the Model 💡✨  \n\nThis section brings together all previous steps to create a **structured process** for prompting the model and obtaining a response. 🚀  \n\n### 🔍 **Steps Explained**  \n\n1️⃣ **Constructing the Prompt 📝**  \n   - A question is defined: *\"What is the capital of France? Use one word.\"* 🌍🏛️  \n   - The `template_without_answer` formats the question to match the expected chat format.  \n\n2️⃣ **Tokenization 🔠➡️🔢**  \n   - The formatted prompt is **converted into numerical tokens** using the tokenizer.  \n   - These tokens are moved to the model’s device (CPU/GPU) for **processing**.  \n\n3️⃣ **Model Prediction 🔥🤖**  \n   - The tokenized input is **passed through the model** in inference mode (`torch.no_grad()` to save memory).  \n   - The model outputs **logits** (raw scores for each possible token).  \n   - These logits are **converted into probabilities** using the softmax function. 📊  \n\n4️⃣ **Selecting the Next Token 🔮**  \n   - The model **predicts the most probable next token** based on the highest probability.  \n\n5️⃣ **Decoding the Prediction 🧩➡️📖**  \n   - The predicted token is **converted back into human-readable text** for the final response.  \n\n✅ **Why This Matters?**  \nThis step ensures that the model generates **accurate and structured responses**, mimicking human-like interactions! 🧠💬  \n","metadata":{"id":"rhLEeQqbiTgu"}},{"cell_type":"code","source":"### Putting it together to prompt the model and generate a response ###\n\n# 1. Construct the prompt in chat template form\nquestion = \"What is the capital of France? Use one word.\"\n# prompt = template_without_answer.format('''TODO''') # TODO\nprompt = template_without_answer.format(question=question)\n\n\n# 2. Tokenize the prompt\ntokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n\n# 3. Feed through the model to predict the next token probabilities\nwith torch.no_grad():\n     # output = '''TODO''' # TODO\n    output = model(tokens)\n\n\n    probs = F.softmax(output.logits, dim=-1)\n\n# 4. Get the next token, according to the maximum probability\nnext_token = torch.argmax(probs[0, -1, :]).item()\n\n# 5. Decode the next token\n# next_token_text = '''TODO''' # TODO\nnext_token_text = tokenizer.decode(next_token)\n\n\nprint(f\"Prompt: {prompt}\")\nprint(f\"Predicted next token: {next_token_text}\")","metadata":{"id":"2SMDd5dpBvnG","outputId":"800c2d76-e419-4a8c-c6c5-cb9ecb37d19c","executionInfo":{"status":"ok","timestamp":1739339024038,"user_tz":-300,"elapsed":1175,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:57:13.698134Z","iopub.execute_input":"2025-02-18T20:57:13.698467Z","iopub.status.idle":"2025-02-18T20:57:14.338027Z","shell.execute_reply.started":"2025-02-18T20:57:13.698439Z","shell.execute_reply":"2025-02-18T20:57:14.337306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model doesn't actually predict the answer to a question; instead, it predicts the next token in a sequence. For more complex questions, a single token isn’t enough—we need to generate an entire sequence of tokens.\n\nTo achieve this, we follow an iterative process: after generating each token, we feed it back into the model to predict the next one. Rather than doing this manually, we can use the model's built-in **model.generate()** function (available in Hugging Face's Transformers library). This allows us to generate a specified number of tokens 'max_new_tokens' and then decode the output back into text.","metadata":{"id":"r6Fx9z5fwiEE"}},{"cell_type":"markdown","source":"*****\n### 🤖 **Generating a Response Using `model.generate`** ✨  \n\nThis step takes a **structured question**, tokenizes it, and feeds it into the model to **generate a complete response**. 🚀  \n\n#### 🔑 **Key Steps:**  \n1️⃣ **Create the Prompt 📝** – Formats *\"What does MIT stand for?\"* using the chat template. 💡🏫  \n2️⃣ **Tokenization 🔠➡️🔢** – Converts text into numerical tokens for the model.  \n3️⃣ **Generate Response 🔮** – Uses `model.generate` to **predict up to 20 new tokens**.  \n4️⃣ **Decode & Display 📖✨** – Converts tokens **back to text** and prints the response.  \n\n✅ **Why This Matters?**  \nUsing `model.generate`, we allow the model to produce **coherent, multi-word responses**, making AI more interactive and useful! 🤖💬  \n*****  \n","metadata":{"id":"Ayr2ezFsjRM7"}},{"cell_type":"code","source":"prompt = template_without_answer.format(question=\"What does MIT stand for?\")\ntokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(tokens, max_new_tokens=20)\nprint(tokenizer.decode(output[0]))","metadata":{"id":"XnWMUQVbBvnG","outputId":"ba12918f-479b-467c-87dc-cbd53b4d65f6","executionInfo":{"status":"ok","timestamp":1739339024823,"user_tz":-300,"elapsed":788,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:57:21.091113Z","iopub.execute_input":"2025-02-18T20:57:21.091439Z","iopub.status.idle":"2025-02-18T20:57:21.782187Z","shell.execute_reply.started":"2025-02-18T20:57:21.091413Z","shell.execute_reply":"2025-02-18T20:57:21.781468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*****\n- The model correctly interprets the prompt and generates a relevant response.  \n- The special tokens ensure that the format remains consistent with the chat template.  \n\nThis step showcases how the model can process user input and generate meaningful answers! 🚀  \n*****","metadata":{"id":"ltH2KHWFjeZX"}},{"cell_type":"markdown","source":"Now we have the basic pipeline for generating text with an LLM!","metadata":{"id":"IgrJ1NS6wiEE"}},{"cell_type":"markdown","source":"## 🍀 **Fine-tuning: Teaching an LLM to Chat Like a Leprechaun!** 🧙‍♂️✨  \n\nFine-tuning lets us **adapt a pre-trained model** to a **specific task, style, or domain** by training it on a **custom dataset**. Instead of building a model from scratch, we **modify an existing LLM’s behavior** to generate **desired responses**.  \n\n### 🛠️ **Why Fine-tune an LLM?**  \n✅ Adapt the model’s **writing style** ✍️  \n✅ Improve **task-specific performance** 🏆  \n✅ Teach the model **new knowledge or abilities** 📚  \n✅ Reduce **biases & unwanted behaviors** ⚖️  \n\n---\n\n### 🍀 **Our Fine-Tuning Task: Leprechaun Chat Style!** 🌈✨  \n\nIn this lab, we will **fine-tune the Gemma LLM** to **speak like a leprechaun**! ☘️  \nWe use a dataset where:  \n- 📜 **Questions** are in standard English.  \n- 🍀 **Answers** mimic a leprechaun’s **playful, rhyming** speech.  \n\n🔹 *Example?* Imagine asking:  \n🗣️ *\"What’s your favorite color?\"*  \n💬 *\"Ah, lad, it’s green, as bright as me gold!\"* 🌟  \n\n![Let’s Dance!](http://33.media.tumblr.com/3d223954ad0a77f4e98a7b87136aa395/tumblr_nlct5lFVbF1qhu7oio1_500.gif)  \n\n---\n\n### 📥 **Loading & Inspecting the Dataset**  \n\nThis step loads **our leprechaun-style dataset** and **examines a sample**!  \n\n#### 🔑 **Key Steps:**  \n1️⃣ **Load Data** – `create_dataloader(style=\"leprechaun\")` loads a dataset where responses follow a *leprechaun-style* writing pattern.  \n2️⃣ **Extract a Sample** – Retrieves the 44th example from the dataset.  \n3️⃣ **Inspect Components:**  \n   - 📝 **Question** – The instruction given to the model.  \n   - 🟢 **Original Answer** – The expected response from the model.  \n   - ✨ **Answer Style** – Describes how the response should be structured.  \n\n💡 **Why This Matters?**  \nBy fine-tuning, we help the model **understand and generate** responses in a **playful, themed way**, making AI conversations more engaging! 🚀  \n********  \n","metadata":{"id":"ZkZKprwcwiEF"}},{"cell_type":"code","source":"train_loader, test_loader = mdl.lab3.create_dataloader(style=\"leprechaun\")\n\n\nsample = train_loader.dataset[44]\nquestion = sample['instruction']\nanswer = sample['response']\nanswer_style = sample['response_style']\n\nprint(f\"Question: {question}\\n\\n\" +\n      f\"Original Answer: {answer}\\n\\n\" +\n      f\"Answer Style: {answer_style}\")","metadata":{"id":"kN0pHHS8BvnH","outputId":"bc69c479-ec74-4cad-e030-43997205fdf0","executionInfo":{"status":"ok","timestamp":1739339527363,"user_tz":-300,"elapsed":3029,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:57:32.819565Z","iopub.execute_input":"2025-02-18T20:57:32.819882Z","iopub.status.idle":"2025-02-18T20:57:34.800228Z","shell.execute_reply.started":"2025-02-18T20:57:32.819856Z","shell.execute_reply":"2025-02-18T20:57:34.799417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This approach helps train models to generate responses in specific tones or writing styles! 🍀✨  \n*********","metadata":{"id":"_aUSvGCylBeE"}},{"cell_type":"code","source":"train_loader = DataLoader(train_loader.dataset, batch_size=1, shuffle=True)  # Reduce batch_size","metadata":{"id":"Ah4C3g0q8Rzw","executionInfo":{"status":"ok","timestamp":1739339532695,"user_tz":-300,"elapsed":392,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:03.846084Z","iopub.execute_input":"2025-02-18T20:58:03.846428Z","iopub.status.idle":"2025-02-18T20:58:03.850350Z","shell.execute_reply.started":"2025-02-18T20:58:03.846398Z","shell.execute_reply":"2025-02-18T20:58:03.849591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(5):  # First 5 samples\n    sample = train_loader.dataset[i]\n    print(sample)\n","metadata":{"id":"WT7O2d-I7AUx","outputId":"4a2fba33-beb2-45b3-e547-ef7c58daf1ae","executionInfo":{"status":"ok","timestamp":1739339534410,"user_tz":-300,"elapsed":360,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:07.942598Z","iopub.execute_input":"2025-02-18T20:58:07.942885Z","iopub.status.idle":"2025-02-18T20:58:07.948968Z","shell.execute_reply.started":"2025-02-18T20:58:07.942862Z","shell.execute_reply":"2025-02-18T20:58:07.948131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*****************\n### 🤖 **Chat Function for Generating AI Responses** 💬✨  \n\nThis function generates **structured responses** from the model based on user input.  \n\n#### 🔍 **How It Works:**  \n1️⃣ **📝 Construct the Prompt** – Formats the question using a **predefined template** for consistency.  \n2️⃣ **🔢 Tokenization** – Converts the text into **model-readable tokens** for processing.  \n3️⃣ **🧠 Generate Response** – Uses the model to **predict and generate** a reply:  \n   - 🎲 `do_sample=True` – Allows variation for more **diverse responses**.  \n   - ✂️ `max_new_tokens=50` – Controls **response length**.  \n   - 🔥 `temperature=0.7` – Adjusts **randomness** (higher = more creative).  \n4️⃣ **📌 Extract the Answer** – If `only_answer=True`, removes **unnecessary prompt text** from the output.  \n5️⃣ **🔡 Decode & Return** – Converts **tokens back into human-readable text**.  \n\n💡 **Why This Matters?**  \nBy structuring prompts and responses, we ensure the AI generates **clear, context-aware** replies while maintaining flexibility. 🚀  \n\n*************\n","metadata":{"id":"TIkQ4vEFwiEF"}},{"cell_type":"code","source":"def chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n    # 1. Construct the prompt using the template\n    # prompt = template_without_answer.format('''TODO''') # TODO\n    prompt = template_without_answer.format(question=question)\n\n\n    # 2. Tokenize the text\n    # input_ids = tokenizer('''TODO''', '''TODO''').to(model.device) # TODO\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n\n    # 3. Feed through the model to predict the next token probabilities\n    with torch.no_grad():\n        # outputs = model.generate('''TODO''', do_sample=True, max_new_tokens=max_new_tokens, temperature=temperature) # TODO\n        outputs = model.generate(**input_ids, do_sample=True,\n        max_new_tokens=max_new_tokens, temperature=temperature)\n\n\n    # 4. Only return the answer if only_answer is True\n    output_tokens = outputs[0]\n    if only_answer:\n        output_tokens = output_tokens[input_ids['input_ids'].shape[1]:]\n\n    # 5. Decode the tokens\n    # result = tokenizer.decode('''TODO''', skip_special_tokens=True) # TODO\n    result = tokenizer.decode(output_tokens, skip_special_tokens=True)\n\n\n    return result\n","metadata":{"id":"d-GfGscMBvnH","executionInfo":{"status":"ok","timestamp":1739339539046,"user_tz":-300,"elapsed":341,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:12.529792Z","iopub.execute_input":"2025-02-18T20:58:12.530075Z","iopub.status.idle":"2025-02-18T20:58:12.535120Z","shell.execute_reply.started":"2025-02-18T20:58:12.530054Z","shell.execute_reply":"2025-02-18T20:58:12.534165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function enables flexible, interactive conversations with the model! 🗣️🤖","metadata":{"id":"uaiNduH8wiEG"}},{"cell_type":"markdown","source":"************\n### 🛠️ **Testing the Chat Function** 🤖💬  \n\nNow, let's test our `chat` function with a **simple question**:  \n\n❓ **Question:** *\"What is the capital of Ireland?\"* 🇮🇪  \n\n#### ✅ **Steps Taken:**  \n1️⃣ Calls the `chat` function with:  \n   - 🎯 `only_answer=True` → Ensures **only the model's answer** is returned.  \n   - ✂️ `max_new_tokens=32` → Limits the **response length** for concise output.  \n2️⃣ 🖨️ **Prints the model's response** to verify accuracy.  \n\n💡 This test checks if the model **correctly understands** and responds **appropriately**. 🚀  \n\n************\n","metadata":{"id":"7ZY22nGrl2hr"}},{"cell_type":"code","source":"# Let's try chatting with the model now to test if it works!\nanswer = chat(\n    \"What is the capital of Ireland?\",\n    only_answer=True,\n    max_new_tokens=32,\n)\n\nprint(answer)\n\n# '''TODO: Experiment with asking the model different questions and temperature values, and see how it responds!'''","metadata":{"id":"FDr5f2djBvnH","outputId":"d6c9b8f4-cfdd-4958-8555-420eb92def9e","executionInfo":{"status":"ok","timestamp":1739339563711,"user_tz":-300,"elapsed":2116,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:16.932441Z","iopub.execute_input":"2025-02-18T20:58:16.932762Z","iopub.status.idle":"2025-02-18T20:58:17.574564Z","shell.execute_reply.started":"2025-02-18T20:58:16.932735Z","shell.execute_reply":"2025-02-18T20:58:17.573833Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🛠️ Next Steps: Experimenting with Temperature\nTo make the responses more diverse, you can experiment with temperature:\n\n###  1️⃣ Low Temperature (More Deterministic)","metadata":{"id":"o9sqZOE-mBcV"}},{"cell_type":"code","source":"print(chat(\"What is the capital of Ireland?\", temperature=0.1, only_answer=True))\n","metadata":{"id":"Q-cVlP2AmOZG","executionInfo":{"status":"ok","timestamp":1739339568800,"user_tz":-300,"elapsed":1048,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"66b0f85b-ea39-4fde-a140-df96ba7f68bf","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:23.698109Z","iopub.execute_input":"2025-02-18T20:58:23.698492Z","iopub.status.idle":"2025-02-18T20:58:24.253602Z","shell.execute_reply.started":"2025-02-18T20:58:23.698460Z","shell.execute_reply":"2025-02-18T20:58:24.252707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2️⃣ High Temperature (More Creative)","metadata":{"id":"7bu9d6H7mR-D"}},{"cell_type":"code","source":"print(chat(\"What is the capital of Ireland?\", temperature=1.5, only_answer=True))\n","metadata":{"id":"r77Z9tV1mWUm","executionInfo":{"status":"ok","timestamp":1739339573316,"user_tz":-300,"elapsed":1153,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"e1832a3e-39d8-4cd7-c7cf-40bbc1f40c8b","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:26.828145Z","iopub.execute_input":"2025-02-18T20:58:26.828491Z","iopub.status.idle":"2025-02-18T20:58:27.383552Z","shell.execute_reply.started":"2025-02-18T20:58:26.828461Z","shell.execute_reply":"2025-02-18T20:58:27.382653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🔥 Try Different Types of Questions\n### Trivia","metadata":{"id":"-z2JKDbNmZ-l"}},{"cell_type":"code","source":"print(chat(\"Who discovered gravity?\", only_answer=True))","metadata":{"id":"5GXWDJR3mgDz","executionInfo":{"status":"ok","timestamp":1739339579555,"user_tz":-300,"elapsed":2007,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"13be5085-c324-410d-9a72-2b8e46bcfadc","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:29.806924Z","iopub.execute_input":"2025-02-18T20:58:29.807248Z","iopub.status.idle":"2025-02-18T20:58:31.381957Z","shell.execute_reply.started":"2025-02-18T20:58:29.807217Z","shell.execute_reply":"2025-02-18T20:58:31.381223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Opinion-Based","metadata":{"id":"1MTm0_nRml20"}},{"cell_type":"code","source":"print(chat(\"What is the best programming language?\", only_answer=True))\n","metadata":{"id":"nejXCZ_Rmrxr","executionInfo":{"status":"ok","timestamp":1739339585078,"user_tz":-300,"elapsed":2094,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"89bfc8e5-4ef9-4837-b255-1f5b5c49a66b","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:31.383037Z","iopub.execute_input":"2025-02-18T20:58:31.383315Z","iopub.status.idle":"2025-02-18T20:58:32.953815Z","shell.execute_reply.started":"2025-02-18T20:58:31.383286Z","shell.execute_reply":"2025-02-18T20:58:32.953112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creative Generation","metadata":{"id":"9IjE_XAjmuaI"}},{"cell_type":"code","source":"print(chat(\"Write a short poem about the moon.\", only_answer=True, max_new_tokens=50))\n","metadata":{"id":"JQrxrzjqmylz","executionInfo":{"status":"ok","timestamp":1739339595998,"user_tz":-300,"elapsed":3022,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"f6e33c26-3d30-419e-d5bd-b5c04487ac35","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:37.778916Z","iopub.execute_input":"2025-02-18T20:58:37.779229Z","iopub.status.idle":"2025-02-18T20:58:40.238241Z","shell.execute_reply.started":"2025-02-18T20:58:37.779201Z","shell.execute_reply":"2025-02-18T20:58:40.237359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**************\n\n### 🚀 **Applying LoRA for Efficient Fine-Tuning** 🧠⚡  \n\nWe use **LoRA (Low-Rank Adaptation)** to fine-tune the model **efficiently** by updating **only a small subset** of parameters instead of the entire model. 🛠️✨  \n\n#### 🔢 **Steps Taken:**  \n1️⃣ **Define LoRA Configuration:**  \n   - 📌 `r=8`: Defines the **rank** of LoRA matrices (controls adaptation complexity).  \n   - 🏷️ `task_type=\"CAUSAL_LM\"`: Specifies LoRA for **causal language modeling**.  \n   - 🎯 `target_modules`: Applies LoRA to **specific model components** (e.g., `q_proj`, `v_proj`).  \n\n2️⃣ **Apply LoRA to the Model:**  \n   - 🏗️ Calls `get_peft_model()` to **integrate LoRA** into the existing model.  \n\n3️⃣ **Check Parameter Efficiency:**  \n   - 📊 Computes and prints the **number of trainable parameters** vs. **total parameters**.  \n   - 📉 Calculates the **percentage of trainable parameters**, showing how LoRA reduces computational costs.  \n\n#### ❓ **Why Use LoRA?**  \n✅ **Reduces memory usage** while maintaining model performance.  \n✅ **Faster fine-tuning** compared to full model updates.  \n✅ **Ideal for resource-constrained environments** like limited GPU setups.  \n\nThis approach ensures **parameter-efficient fine-tuning** while leveraging the strengths of **large language models**. 🚀💡  \n\n************\n","metadata":{"id":"HDqdcI8AwiEH"}},{"cell_type":"code","source":"# LoRA is a way to finetune LLMs very efficiently by only updating a small subset of the model's parameters\n\ndef apply_lora(model):\n    # Define LoRA config\n    lora_config = LoraConfig(\n        r=8, # rank of the LoRA matrices\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\n            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n        ],\n    )\n\n    # Apply LoRA to the model\n    lora_model = get_peft_model(model, lora_config)\n    return lora_model\n\nmodel = apply_lora(model)\n\n# Print the number of trainable parameters after applying LoRA\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"number of trainable parameters: {trainable_params}\")\nprint(f\"total parameters: {total_params}\")\nprint(f\"percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")","metadata":{"id":"Fb6Y679hBvnI","outputId":"97c71bc9-5c1e-45d9-c8ea-8634e35d81ff","executionInfo":{"status":"ok","timestamp":1739339601803,"user_tz":-300,"elapsed":357,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:58:43.543773Z","iopub.execute_input":"2025-02-18T20:58:43.544100Z","iopub.status.idle":"2025-02-18T20:58:43.854944Z","shell.execute_reply.started":"2025-02-18T20:58:43.544071Z","shell.execute_reply":"2025-02-18T20:58:43.854201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"------------\n### ⚡ **Forward Pass and Loss Computation** 🧠📉  \n\nThis function performs a **forward pass** through the model and computes the **loss** for training. 🎯  \n\n#### 🔢 **Steps Taken:**  \n1️⃣ **Truncate Input to Context Length:**  \n   - ✂️ Ensures that both `tokens` and `mask` are within the defined `context_length` (default: 512).  \n\n2️⃣ **Prepare Input and Output:**  \n   - 🔹 `x`: Input tokens (excluding the last token).  \n   - 🔹 `y`: Target tokens (excluding the first token).  \n   - 🔹 `mask`: Adjusted to align with `y` to avoid computing loss on padding tokens.  \n\n3️⃣ **Forward Pass:**  \n   - 🚀 The model processes `x` to generate **logits** (predicted token probabilities).  \n\n4️⃣ **Compute Cross-Entropy Loss:**  \n   - 📉 Loss is computed using `F.cross_entropy()`, comparing predicted logits with actual tokens.  \n   - 📝 `reduction=\"none\"` ensures **per-token** loss calculation.  \n\n5️⃣ **Apply Masking:**  \n   - 🎭 Loss is only considered for **valid tokens** (excluding padding).  \n   - 🔄 The final loss is **averaged across valid tokens** for efficiency.  \n\n#### ❓ **Why Is This Important?**  \n✅ **Prevents unnecessary computation** on padding tokens.  \n✅ **Ensures loss calculation is focused** on relevant tokens.  \n✅ **Optimizes training efficiency** and improves model learning.  \n\n🚀 This approach enhances the model’s ability to learn efficiently while minimizing computation overhead.  \n\n------------\n","metadata":{"id":"32ryAA-6wiEI"}},{"cell_type":"code","source":"def forward_and_compute_loss(model, tokens, mask, context_length=512):\n    # Truncate to context length\n    tokens = tokens[:, :context_length]\n    mask = mask[:, :context_length]\n\n    # Construct the input, output, and mask\n    x = tokens[:, :-1]\n    y = tokens[:, 1:]\n    mask = mask[:, 1:]\n\n    # Forward pass to compute logits\n    logits = model(x).logits\n\n    # Compute loss\n    loss = F.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        y.view(-1),\n        reduction=\"none\"\n    )\n\n    # Mask out the loss for non-answer tokens\n    loss = loss[mask.view(-1)].mean()\n\n    return loss","metadata":{"id":"xCLtZwxwBvnI","executionInfo":{"status":"ok","timestamp":1739339606765,"user_tz":-300,"elapsed":349,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:12:30.975101Z","iopub.execute_input":"2025-02-18T21:12:30.975488Z","iopub.status.idle":"2025-02-18T21:12:30.981142Z","shell.execute_reply.started":"2025-02-18T21:12:30.975446Z","shell.execute_reply":"2025-02-18T21:12:30.980166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**********\n### 🚀 **Training Loop for Fine-Tuning** 🎯🔥  \n\nThis function **trains** the model using **LoRA** for efficient fine-tuning. 🧠✨  \n\n#### 🔑 **Key Steps:**  \n\n1️⃣ **Apply LoRA to the Model:**  \n   - 🎯 Reduces trainable parameters while maintaining performance.  \n\n2️⃣ **Initialize the Optimizer:**  \n   - 🚀 Uses the **Lion** optimizer with a learning rate of **1e-4**.  \n   - ⚡ Efficient for training large language models.  \n\n3️⃣ **Iterate Through the DataLoader:**  \n   - 📥 Extracts a **question** and its **styled answer** from the batch.  \n   - 📝 Formats them into a structured **prompt** using a predefined template.  \n\n4️⃣ **Tokenization and Masking:**  \n   - 🔡 Converts text into token IDs for model processing.  \n   - 🎭 Generates a **mask** to focus loss computation only on answer tokens.  \n\n5️⃣ **Forward and Loss Computation:**  \n   - 🚀 Passes tokens through the model.  \n   - 📉 Computes **loss** using the masked token predictions.  \n\n6️⃣ **Backward Pass and Optimization:**  \n   - 🔄 **Clears gradients** from the optimizer.  \n   - 📊 **Computes gradients** using `loss.backward()`.  \n   - 🔧 **Updates model parameters** with `optimizer.step()`.  \n\n7️⃣ **Monitoring Training Progress:**  \n   - 👀 Every **10 steps**, the model answers a test question (`What is the capital of France?`).  \n   - 📈 The average loss over recent steps is printed for tracking improvements.  \n\n8️⃣ **Early Stopping:**  \n   - 🛑 Stops training when `max_steps` is reached.  \n\n#### ❓ **Why This Matters?**  \n✅ **Efficient fine-tuning** using LoRA.  \n✅ **Selective loss computation** improves training quality.  \n✅ **Monitors performance** during training to ensure improvements.  \n\n🚀 This ensures a streamlined, efficient, and well-monitored fine-tuning process.  \n\n**********\n","metadata":{"id":"zrjCQSHmwiEI"}},{"cell_type":"code","source":"### Training loop ###\n\ndef train(model, dataloader, tokenizer, max_steps=200, context_length=512, learning_rate=1e-4):\n    losses = []\n\n    # Apply LoRA to the model\n    # model = '''TODO''' # TODO\n    model = apply_lora(model)\n\n\n    optimizer = Lion(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for step, batch in enumerate(dataloader):\n        question = batch[\"instruction\"][0]\n        answer = batch[\"response_style\"][0]\n\n        # Format the question and answer into the template\n        # text = template_with_answer.format('''TODO''', '''TODO''') # TODO\n        text = template_with_answer.format(question=question, answer=answer)\n\n\n        # Tokenize the text and compute the mask for the answer\n        ids = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True).to(model.device)\n        mask = ids[\"offset_mapping\"][:,:,0] >= text.index(answer)\n\n        # Feed the tokens through the model and compute the loss\n        # loss = forward_and_compute_loss('''TODO''') # TODO\n        loss = forward_and_compute_loss(\n            model=model,\n            tokens=ids[\"input_ids\"],\n            mask=mask,\n            context_length=context_length,\n        )\n\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        # monitor progress\n        if step % 10 == 0:\n            print(chat(\"What is the capital of France?\", only_answer=True))\n            print(f\"step {step} loss: {torch.mean(torch.tensor(losses)).item()}\")\n            losses = []\n\n        if step > 0 and step % max_steps == 0:\n            break\n\n    return model\n","metadata":{"id":"JfiIrH7jBvnI","executionInfo":{"status":"ok","timestamp":1739339612213,"user_tz":-300,"elapsed":554,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:12:32.465907Z","iopub.execute_input":"2025-02-18T21:12:32.466207Z","iopub.status.idle":"2025-02-18T21:12:32.472804Z","shell.execute_reply.started":"2025-02-18T21:12:32.466180Z","shell.execute_reply":"2025-02-18T21:12:32.471997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🚀 **Fine-Tuning the Model** 🧠🔥  \n\n#### 🔑 **Key Steps:**  \n\n1️⃣ **Clear GPU Cache** 🧹  \n   - Calls `torch.cuda.empty_cache()` to **free up memory**.  \n   - Ensures **efficient memory usage** before training starts.  \n\n2️⃣ **Start the Training Process** 🎯  \n   - Calls the `train()` function with:  \n     - 🏋️ **Pre-trained model** (with LoRA applied).  \n     - 📚 **Training dataset (train_loader)**.  \n     - 🔡 **Tokenizer** for text processing.  \n     - ⏳ **Max Steps:** 50 (Stops training after 50 iterations).  \n     - 📝 **Context Length:** 256 (Limits the number of tokens per input).  \n\n🚀 This ensures **optimized fine-tuning** while efficiently managing resources!  \n\n***********\n","metadata":{"id":"QKGbwmhpoTgx"}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"id":"yIG4fm1J55tx","executionInfo":{"status":"ok","timestamp":1739339615018,"user_tz":-300,"elapsed":323,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:12:40.253239Z","iopub.execute_input":"2025-02-18T21:12:40.253614Z","iopub.status.idle":"2025-02-18T21:12:40.257455Z","shell.execute_reply.started":"2025-02-18T21:12:40.253586Z","shell.execute_reply":"2025-02-18T21:12:40.256541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the train function to fine-tune the model! Hint: you'll start to see results after a few dozen steps.\n# model = train('''TODO''') # TODO\nmodel = train(model, train_loader, tokenizer, max_steps=50, context_length=256)\n","metadata":{"id":"blFoO-PhBvnI","outputId":"aea07cb0-7bb8-4838-fbec-5da8c6cd730a","executionInfo":{"status":"ok","timestamp":1739339669381,"user_tz":-300,"elapsed":53265,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:12:41.457671Z","iopub.execute_input":"2025-02-18T21:12:41.457991Z","iopub.status.idle":"2025-02-18T21:13:14.422471Z","shell.execute_reply.started":"2025-02-18T21:12:41.457964Z","shell.execute_reply":"2025-02-18T21:13:14.421569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"🎾 **Let's Try Chatting with the Model Again!** 🗨️🤖  \n\n### 📝 **Generating a Model Response**  \n- The model is prompted with:  \n  **❓ \"What is a good story about tennis?\"** 🎾  \n- It generates a **200-token** response based on fine-tuning. ✍️📖  \n- This tests:  \n  ✅ **Storytelling ability** 📚  \n  ✅ **Fine-tuned knowledge** 🏆  \n\n🚀 Let's see how our model has evolved!  \n","metadata":{"id":"YB15znN0wiEK"}},{"cell_type":"code","source":"print(chat(\"What is a good story about tennis\", only_answer=True, max_new_tokens=200))","metadata":{"id":"su4ZAG3eBvnI","outputId":"7889095f-5aec-471e-b259-db0f38d13a02","executionInfo":{"status":"ok","timestamp":1739339690353,"user_tz":-300,"elapsed":15717,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:13:23.356813Z","iopub.execute_input":"2025-02-18T21:13:23.357160Z","iopub.status.idle":"2025-02-18T21:13:37.346515Z","shell.execute_reply.started":"2025-02-18T21:13:23.357130Z","shell.execute_reply":"2025-02-18T21:13:37.345576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*********************************\n*******************************","metadata":{"id":"xyx3EOhqoqxY"}},{"cell_type":"markdown","source":"🌟 **Evaluating a Style-Tuned LLM** 🍀🤖  \n\n### 🧐 **How Can I Tell If the Model Is Performing Well?**  \nHow closely does its style match that of a **leprechaun**? 🍀✨  \nEvaluating generated responses can feel **subjective**, making it tricky to measure performance.  \n\n### 🚀 **Challenges in Evaluation**  \n- **Benchmarks** don’t always reflect real-world performance. 📊  \n- A model may score well but **struggle in practical use cases**. 🤔  \n- Some benchmarks might contain **data from the model’s training set**. 🔄  \n\n### 🔬 **Structured Evaluation with \"LLM as a Judge\"**  \nTo **quantify performance**, I’ll use a **larger LLM** to evaluate responses from my fine-tuned model. 👨‍⚖️⚖️  \n\n#### **How It Works:**  \n1. The **larger LLM** acts as a **judge**, scoring the smaller model’s outputs. 🏆  \n2. A **system prompt** guides the judge, defining:  \n   ✅ The **task** 🎯  \n   ✅ The **evaluation criteria** ✅  \n3. The **judge LLM assigns scores**, turning subjective evaluation into **measurable results**. 📈  \n\n✨ This approach makes it easier to assess how well the model **captures the leprechaun style!** 🍀  \n","metadata":{"id":"2cvhTsptBvnI"}},{"cell_type":"markdown","source":"## 🌟 **Fine-Tune Well, I Must!** 🧙‍♂️💫  \n\nMy **leprechaun-tuned** model is already speaking in a magical Irish style—must be the **luck of the Irish!** 🍀✨  \n\nNow, let’s take things up a notch by fine-tuning my model to **speak like Yoda** from *Star Wars*! 🤖🔁🛸  \n\n![Wise, he is!](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExZHcxMGZjZzdwbGV0andseWw3c3h1ODJwOXd5NHEzbnVtMHk5YWQyayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/IaWMz9Ln8OWvf66z6k/giphy.webp)  \n\n### 🎯 **Mission: Train the Model in Yoda’s Unique Speech Pattern**  \n- Fine-tune responses so the model speaks **like the wise Jedi Master**. 🟢🌀  \n- Use **LLM as a Judge** to **evaluate style accuracy**. 📏👨‍⚖️  \n- Analyze feedback and refine the approach until the model is **one with the Force**! 🌌  \n\n#### **Success, I Seek!** 💪  \nStrong in the **Force**, my model will be! 🚀🌠  \n","metadata":{"id":"CzMYR66vwiEK"}},{"cell_type":"code","source":"# Load the Yoda-speak dataset and fine-tune the model using your training function\ntrain_loader, test_loader = mdl.lab3.create_dataloader(style=\"yoda\")\n\n# model = train('''TODO''') # TODO\nmodel = train(model, train_loader, tokenizer, max_steps=50)\n","metadata":{"id":"-gLgE41YBvnJ","outputId":"dab7118e-bc63-4b7e-f5db-7d8502348892","executionInfo":{"status":"ok","timestamp":1739339820385,"user_tz":-300,"elapsed":33474,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:13:53.506912Z","iopub.execute_input":"2025-02-18T21:13:53.507230Z","iopub.status.idle":"2025-02-18T21:14:15.117472Z","shell.execute_reply.started":"2025-02-18T21:13:53.507202Z","shell.execute_reply":"2025-02-18T21:14:15.116637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ⚖️ **LLM as a Judge: Evaluating Text Style** 🤖📜  \n\n- The **LLM acts as an impartial judge**, analyzing if a response matches a **specific style**. ⚡  \n- A **system prompt** defines the expected writing style, such as **Yoda’s unique speech pattern**. 🟢🌀  \n- The judge **critically evaluates** a response and assigns a **score from 0 to 10**. 🔢📊  \n- The output must follow a **strict JSON format** with a `\"score\"` key. 📂💾  \n\n#### **✨ Why This Matters?**  \n✅ Helps measure **style consistency** objectively.  \n✅ Provides **quantifiable feedback** on fine-tuning effectiveness.  \n✅ Enables iterative **improvements** to style adaptation.  \n\n🚀 **Judge wisely, the LLM must!** 🧙‍♂️🔍  \n","metadata":{"id":"zptC2ZRKwiEL"}},{"cell_type":"code","source":"### LLM as a judge ###\n\n'''TODO: Experiment with different system prompts to see how they affect the judge LLM's evaluation!\n        Come back to this cell after you've generated some text from your model.'''\n\nsystem_prompt = \"\"\"\nYou are an impartial judge that evaluates if text was written by {style}.\n\nAn example piece of text from {style} is:\n{example}\n\nNow, analyze some new text carefully and respond on if it follows the\nsame style of {style}. Be critical to identify any issues in the text.\nThen convert your feedback into a number between 0 and 10: 10 if the text\nis written exactly in the style of {style}, 5 if mixed faithfulness to the\nstyle, or 0 if the text is not at all written in the style of {style}.\n\nThe format of the your response should be a JSON dictionary and nothing else:\n{{\"score\": <score between 0 and 10>}}\n\"\"\"\n\nstyle = \"Yoda\"\n# example = \"\"\"The very Republic is threatened, if involved the Sith are. Hard to see, the dark side is. \"\"\"\nexample = \"The very Republic is threatened, if involved the Sith are. Hard to see, the dark side is. Discover who this assassin is, we must. With this Naboo queen you must stay, Qui-Gon. Protect her. May the Force be with you. A vergence, you say? But you do! Revealed your opinion is. Trained as a Jedi, you request for him? Good, good, young one.\"\n\nsystem_prompt = system_prompt.format(style=style, example=example)\nprint(\"=== System prompt ===\")\nprint(system_prompt)","metadata":{"id":"REkrJ1SCBvnJ","outputId":"b495a77a-85e9-467b-f942-b0a71354ddf5","executionInfo":{"status":"ok","timestamp":1739339848605,"user_tz":-300,"elapsed":357,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:14:22.800340Z","iopub.execute_input":"2025-02-18T21:14:22.800650Z","iopub.status.idle":"2025-02-18T21:14:22.806712Z","shell.execute_reply.started":"2025-02-18T21:14:22.800623Z","shell.execute_reply":"2025-02-18T21:14:22.805913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🔧 **Setting Up the LLM Client with OpenRouter** 🌐🤖  \n\n- A **🔑 API key** is required to connect to **OpenRouter** and access the model. 🔗  \n- The selected model is **\"liquid/lfm-40b\"**, but alternatives like **\"google/gemma-2-9b-it\"** can also be used. 🏗️📚  \n- A **security check** ensures the API key is **not empty** before proceeding. ✅🔒  \n- The `LLMClient` is initialized with the chosen model and API credentials. 🖥️📡  \n- This setup enables seamless **interaction with OpenRouter’s API** for LLM-powered tasks. 🚀  \n\n⚡ **Connected to the Force, your model is!** 🧙‍♂️🌟  \n","metadata":{"id":"UYXaAXRmwiEM"}},{"cell_type":"code","source":"import os\n\n# Retrieve the API key from Colab Secrets\n#OPENROUTER_API_KEY = os.getenv(\"OPEN_ROUTER_API_KEY\")\n#assert OPENROUTER_API_KEY, \"You must set your OpenRouter API key in Colab Secrets before running this cell!\"\n\n# Retrieve the API key from Kaggle Secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENROUTER_API_KEY = user_secrets.get_secret(\"OPEN_ROUTER_API_KEY\")\n\nassert OPENROUTER_API_KEY, \"You must set your OpenRouter API key in Kaggle Secrets before running this cell!\"\n\n\nmodel_name = \"liquid/lfm-40b\"\n# model_name = \"google/gemma-2-9b-it\"\nllm = mdl.lab3.LLMClient(model=model_name, api_key=OPENROUTER_API_KEY)","metadata":{"id":"9S7DtGZ5BvnJ","executionInfo":{"status":"ok","timestamp":1739341910390,"user_tz":-300,"elapsed":5,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:17:09.814486Z","iopub.execute_input":"2025-02-18T21:17:09.814837Z","iopub.status.idle":"2025-02-18T21:17:10.340705Z","shell.execute_reply.started":"2025-02-18T21:17:09.814811Z","shell.execute_reply":"2025-02-18T21:17:10.339840Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*************  \n\n### **🧠 LLM-Based Evaluation System Summary**  \n`LLMJudgeEvaluator` uses a **🤖 language model (LLM) as a judge** to evaluate text based on a **📜 predefined system prompt**.  \n\n#### **⚙️ Evaluation Process**  \n1. **📝 Prompt Formatting** – The input text is inserted into an evaluation template.  \n2. **⚖️ LLM Evaluation** – The model generates a **📊 JSON-formatted score (0-10)** with a stopping condition (`stop=[\"}\"]`).  \n3. **📉 Score Extraction & Normalization** – The score is **🔢 converted, 🔄 normalized (0-1), and ✂️ clipped** to valid limits.  \n4. **🛠️ Error Handling** – **🔁 Up to 20 retries** ensure robustness before raising an exception.  \n\n#### **✨ Key Benefits**  \n- **🤖 Automates evaluation** with LLM-based judgment.  \n- **📈 Provides flexible and scalable scoring.**  \n- **🛡️ Ensures reliability with error handling.**  \n\n*************  \n","metadata":{"id":"QrukkAfNwiEM"}},{"cell_type":"code","source":"from opik.evaluation.metrics import base_metric, score_result\n\nclass LLMJudgeEvaluator(base_metric.BaseMetric):\n    def __init__(self, judge: mdl.lab3.LLMClient = None, system_prompt: str = None):\n        self.judge = judge\n        self.system_prompt = system_prompt\n        self.prompt_template = \"Evaluate this text: {text}\"\n\n    def score(self, text: str, n_tries=20, **kwargs):\n        \"\"\" Evaluate by asking an LLM to score it. \"\"\"\n\n        for attempt in range(n_tries):\n            try:\n                # TODO: Convert the text to template form before passing it to the judge LLM\n                prompt = self.prompt_template.format(text=text)\n                # prompt = self.prompt_template.format('''TODO''') # TODO\n\n                # The system prompt asks the judge to output a JSON dictionary of the form:\n                # {\"score\": <score between 0 and 10>}\n                # To do this, we need to specify the judge to stop generating after it\n                # closes the JSON dictionary (i.e., when it outputs \"}\")\n                # Hint: Use the stop=[\"}\"] argument within the judge.ask() method to specify this.\n                stop = \"}\"\n\n                # TODO: Call the judge LLM with the system prompt and the prompt template.\n                # Remember to stop the generation when the judge LLM outputs \"}\".\n                 # res = self.judge.ask(\n                #   system='''TODO''',\n                #   user='''TODO''',\n                #   max_tokens='''TODO'''\n                #   stop='''TODO'''\n                # ) # TODO\n                res = self.judge.ask(\n                    system=self.system_prompt,\n                    user=prompt,\n                    max_tokens=10,\n                    stop=[stop]\n                )\n\n                # Extract the assistant's content from the API response\n                # Remember to add the stop character back to the end of the response to be a\n                # valid JSON dictionary (its not there  the judge LLM stoped once it saw it)\n                res = res.choices[0].message.content + stop\n                res_dict = json.loads(res)\n\n                max_score = 10 # The maximum score that the LLM should output\n                score = res_dict[\"score\"] / max_score # Normalize\n                score = max(0.0, min(score, 1.0)) # Clip between 0 and 1\n\n                # Return the score object\n                return score_result.ScoreResult(name=\"StyleScore\", value=score)\n\n            except Exception as e:\n                if attempt == n_tries - 1:  # Last attempt\n                    raise e  # Re-raise the exception if all attempts failed\n                continue  # Try again if not the last attempt","metadata":{"id":"llB3FgiwBvnJ","executionInfo":{"status":"ok","timestamp":1739342108438,"user_tz":-300,"elapsed":357,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:07.364774Z","iopub.execute_input":"2025-02-18T21:22:07.365057Z","iopub.status.idle":"2025-02-18T21:22:07.372152Z","shell.execute_reply.started":"2025-02-18T21:22:07.365033Z","shell.execute_reply":"2025-02-18T21:22:07.371357Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Instaniate your Comet Opik judge using the LLMJudgeEvaluator class and system prompt.","metadata":{"id":"_c_7p_eowiEM"}},{"cell_type":"markdown","source":"*************  \n\n### **🚀 Initializing the LLM Judge**  \nThe `LLMJudgeEvaluator` is instantiated using:  \n\n- **🤖 `llm`** → The **language model** that will act as the evaluator.  \n- **📝 `system_prompt`** → A **predefined instruction** guiding the model’s judgment process.  \n\nThis setup ensures **✅ consistent and 🔄 automated** evaluation of text based on the given style.  \n\n*************  \n","metadata":{"id":"1MTq1fvuyODS"}},{"cell_type":"code","source":"judge = LLMJudgeEvaluator(llm, system_prompt=system_prompt)","metadata":{"id":"ejuSiiKDwiEN","executionInfo":{"status":"ok","timestamp":1739342111868,"user_tz":-300,"elapsed":337,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:17:38.906159Z","iopub.execute_input":"2025-02-18T21:17:38.906472Z","iopub.status.idle":"2025-02-18T21:17:38.910231Z","shell.execute_reply.started":"2025-02-18T21:17:38.906447Z","shell.execute_reply":"2025-02-18T21:17:38.909328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*************  \n\n### **📊 Scoring Text Using the LLM Judge**  \n\n#### **📝 Overview**  \n- The `scoring_function` uses the **🤖 LLM judge** to evaluate text.  \n- It returns a **🔢 normalized score** (0 to 1) based on how well the text matches a predefined style.  \n\n#### **⚙️ Process**  \n1. **📨 Pass the text** to the judge for evaluation.  \n2. **🔍 Retrieve the score** from the judge's response.  \n3. **📊 Print results** for different test texts.  \n\nThis process **✅ quantifies style similarity**, making it useful for **📈 automated text evaluation**.  \n\n*************  \n","metadata":{"id":"rcNw55ZwwiEN"}},{"cell_type":"code","source":"def scoring_function(text):\n    return judge.score(text).value\n\ntest_texts = [\n    \"Tennis is a fun sport. But you must concentrate.\",\n    \"Fun sport, tennis is. But work hard, you must.\",\n    \"Hard to see, the dark side is.\"\n]\n\nfor text in test_texts:\n    score = scoring_function(text)\n    print(f\"{text} ==> Score: {score}\")","metadata":{"id":"D_rvQDrvBvnJ","outputId":"88295299-b4d5-4d28-9041-9ae2fffbf76e","executionInfo":{"status":"ok","timestamp":1739342116452,"user_tz":-300,"elapsed":1952,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:17:41.070954Z","iopub.execute_input":"2025-02-18T21:17:41.071243Z","iopub.status.idle":"2025-02-18T21:17:42.365619Z","shell.execute_reply.started":"2025-02-18T21:17:41.071220Z","shell.execute_reply":"2025-02-18T21:17:42.364755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***************  \n\n### **📊 LLM-Based Style Scoring Results**  \n\n- **🔴 0.067 Score** → Regular sentence structure, **no Yoda-like style**.  \n- **🟡 0.7 Score** → Partial inversion, **somewhat resembles** Yoda’s speech.  \n- **🟢 1.0 Score** → Fully inverted syntax, **perfectly matches** Yoda’s style.  \n\n✅ Higher scores indicate **greater similarity** to Yoda’s way of speaking.  \n\n***************  \n","metadata":{"id":"Mp2dX2lvzRoJ"}},{"cell_type":"markdown","source":"To evaluate how well my fine-tuned model is performing, I'll score its outputs alongside two reference points:\n\nNegative control: The base-style text (which hasn’t been fine-tuned).\nPositive control: Text from the training set in Yoda-speak.\nI'll generate responses from my model by asking it new questions and comparing its style to Yoda’s speech pattern.","metadata":{"id":"4z-dJ4ztwiEN"}},{"cell_type":"code","source":"# Generate text from your model by asking it new questions.\ndef generate_samples_from_test(test_loader, num_samples):\n    samples = []\n    for test_sample in tqdm(test_loader, total=num_samples):\n        test_question = test_sample['instruction'][0]\n        with torch.no_grad():\n            generated = chat(test_question, only_answer=True, max_new_tokens=100)\n        samples.append(generated)\n        if len(samples) >= num_samples:\n            break\n    return samples\n\nn_samples = 20\ngenerated_samples = generate_samples_from_test(test_loader, num_samples=n_samples)","metadata":{"id":"9tzp4HPZBvnJ","outputId":"1b64cea1-4e3e-4f7d-a7d5-db7a72602b21","executionInfo":{"status":"ok","timestamp":1739342192190,"user_tz":-300,"elapsed":72981,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:17:54.794838Z","iopub.execute_input":"2025-02-18T21:17:54.795134Z","iopub.status.idle":"2025-02-18T21:18:40.012083Z","shell.execute_reply.started":"2025-02-18T21:17:54.795111Z","shell.execute_reply":"2025-02-18T21:18:40.011372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's also collect some base-style text (`base_samples`) and the training-set text in Yoda-speak style (`style_samples`). For these, we won't need to generate text, since we already have the text in the dataset.","metadata":{"id":"0wGGEpt7wiEO"}},{"cell_type":"code","source":"base_samples = [sample['response'][0] for i, sample in enumerate(train_loader) if i < n_samples]\nstyle_samples = [sample['response_style'][0] for i, sample in enumerate(train_loader) if i < n_samples]","metadata":{"id":"ZEpUWV2EBvnK","executionInfo":{"status":"ok","timestamp":1739342196739,"user_tz":-300,"elapsed":704,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:18:50.809707Z","iopub.execute_input":"2025-02-18T21:18:50.810017Z","iopub.status.idle":"2025-02-18T21:18:51.366555Z","shell.execute_reply.started":"2025-02-18T21:18:50.809994Z","shell.execute_reply":"2025-02-18T21:18:51.365900Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have our samples, we can score them using the judge LLM. We will use a multiprocessed scoring function to score the samples in parallel, because each sample is independent and we can submit them all as simultaneous requests to the judge LLM.","metadata":{"id":"xXaWUnsbwiEO"}},{"cell_type":"code","source":"# Create a multiprocessed scoring function to score the samples in parallel\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom multiprocessing import Pool\n\ndef compute_scores_in_parallel(samples):\n    with Pool(processes=10) as pool:\n        scores = pool.map(scoring_function, samples)\n    return scores\n\n# Compute and print the scores for the base-style text, generated text, and training-set text in Yoda-speak style\nbase_scores = compute_scores_in_parallel(base_samples)\nprint(f\"Base: {np.mean(base_scores):.2f} ± {np.std(base_scores):.2f}\")\n\ngenerated_scores = compute_scores_in_parallel(generated_samples)\nprint(f\"Gen: {np.mean(generated_scores):.2f} ± {np.std(generated_scores):.2f}\")\n\nstyle_scores = compute_scores_in_parallel(style_samples)\nprint(f\"Train: {np.mean(style_scores):.2f} ± {np.std(style_scores):.2f}\")","metadata":{"id":"2X6MNQc3BvnK","outputId":"5c0b319b-26ad-47dc-c307-3f7da74579ee","executionInfo":{"status":"ok","timestamp":1739342205856,"user_tz":-300,"elapsed":7385,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:20:45.674067Z","iopub.execute_input":"2025-02-18T21:20:45.674426Z","iopub.status.idle":"2025-02-18T21:20:51.640999Z","shell.execute_reply.started":"2025-02-18T21:20:45.674393Z","shell.execute_reply":"2025-02-18T21:20:51.639941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***********  \n\n### **📊 Evaluation Scores**  \n\nThe performance of my fine-tuned model is evaluated using different text sources:  \n\n- 🟢 **Base Model:** `0.12 ± 0.23` _(Original model without fine-tuning)_  \n- 🟡 **Generated Text:** `0.21 ± 0.24` _(Output from my fine-tuned model)_  \n- 🔴 **Training Set Text:** `0.39 ± 0.27` _(Yoda-style text from the dataset)_  \n\n✅ The generated text (**Gen**) closely aligns with the training set (**Train**),  \nsuggesting that the model has effectively learned Yoda’s speech style.  \n\n***********  \n","metadata":{"id":"bP_PyD09zf9A"}},{"cell_type":"code","source":"# Ignore deprecation warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also plot the distribution of scores for each of the three types of text.\n","metadata":{"id":"lF1F_ehRwiEP"}},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\n\n# Create clean DataFrame\ndf = pd.DataFrame({\n    'Score': [*base_scores, *generated_scores, *style_scores],\n    'Type': ['Base']*len(base_scores) + ['Generated']*len(generated_scores) + ['Style']*len(style_scores)\n})\n\n# Plot with seaborn\nsns.histplot(data=df, x='Score', hue='Type', multiple=\"dodge\", bins=6, shrink=.8)\n\nplt.title('Distribution of Scores')\nplt.show()","metadata":{"id":"V4-g0Z3_BvnK","outputId":"a00a6170-6932-4610-9e47-2dcf2bac966d","executionInfo":{"status":"ok","timestamp":1739341067671,"user_tz":-300,"elapsed":858,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:18.293029Z","iopub.execute_input":"2025-02-18T21:22:18.293356Z","iopub.status.idle":"2025-02-18T21:22:18.545133Z","shell.execute_reply.started":"2025-02-18T21:22:18.293327Z","shell.execute_reply":"2025-02-18T21:22:18.544191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use these observations to improve your model. Remember that the judge LLM is not perfect, and you can try to improve the judge LLM to better evaluate the model's outputs. A better judge LLM will give you a better evaluation of how well your Yoda model is doing, and that better evaluation will help you improve your Yoda model.","metadata":{"id":"GSRPSFs3wiEP"}},{"cell_type":"markdown","source":"## **2.5: 🎯 Conclusion**  \n\nI experimented with both my **chat model** and the **judge LLM** to enhance the quality of Yoda-speak.  \nThe goal was to fine-tune my model to better understand and generate text in Yoda’s unique speech style.  \n\n### **📌 Evaluation Criteria**  \n\n- 🧠 **Likelihood of true Yoda-speak under my chat model:**  \n  - A well-trained model should estimate a **lower cross-entropy loss** for text that truly follows Yoda’s speech pattern.  \n  - At the end of this lab, I will evaluate the likelihood of a **held-out test sample** of Yoda-speak under my model and include it in my report.  \n  - This will provide a **quantitative measure** to compare different chat models (which may have been influenced by different judge LLMs).  \n\n- 🔍 **Experiments and modifications:**  \n  - I documented the **various changes** I made to improve my model.  \n  - I analyzed their **impact** and observed how they affected the quality of Yoda-style responses.  \n\n### **🚀 Next Steps**  \nI will now run the evaluation cell below to print the **final results**.  \n\n**🔴 Note:** I will **not** modify the content of the evaluation cell as instructed.  \n","metadata":{"id":"CzehpdXjwiEQ"}},{"cell_type":"code","source":"# DO NOT CHANGE/MODIFY THIS CELL.\n# EXECUTE IT BEFORE SUBMITTING YOUR ENTRY TO THE LAB.\n\nyoda_test_text = mdl.lab3.yoda_test_text\ntokens = tokenizer(yoda_test_text, return_tensors=\"pt\").to(model.device)\n\n# Get the loglikelihood from the model\nwith torch.no_grad():\n    outputs = model(**tokens)\n    logits = outputs.logits[:, :-1]\n    targets = tokens.input_ids[:, 1:]\n    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n                            targets.reshape(-1))\n\nprint(f\"Yoda test loglikelihood: {loss.item():.2f}\")\n","metadata":{"id":"MqnrG24FBvnK","outputId":"f6e7df8c-d9a6-43a6-8d77-9047ab68f884","executionInfo":{"status":"ok","timestamp":1739334622469,"user_tz":-300,"elapsed":712,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:23.768848Z","iopub.execute_input":"2025-02-18T21:22:23.769173Z","iopub.status.idle":"2025-02-18T21:22:24.003723Z","shell.execute_reply.started":"2025-02-18T21:22:23.769143Z","shell.execute_reply":"2025-02-18T21:22:24.002849Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****************************\n****************************","metadata":{"id":"k3ixV0xGwiEQ"}},{"cell_type":"markdown","source":"### ✨ **LLM-Based Shakespearean Style Judge** 🎭  \naS we are asked to try the process with other style so, lets try this\n\n#### 📜 **Overview**  \n- The **LLM judge** 🧑‍⚖️ evaluates if a given text follows the **Shakespearean** 🎭 writing style.  \n- The **system prompt** provides context and an example from Shakespeare’s works 📖.  \n- The model returns a **JSON-formatted score (0-10) 🔢** indicating style similarity.  \n\n#### 🏰 **Evaluation Process**  \n1. **Defining the System Prompt** 📝  \n2. **Scoring Criteria** 🎯 \n3. **📊 Test Samples for Evaluation**  \n4. **📈 Computing the Scores**  \n\n***********  \n","metadata":{"id":"w3zfuFbZ55N6"}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom multiprocessing import Pool\n\n# Disable tokenizer parallelism (only needed for some models like Hugging Face transformers)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Define system prompt for Shakespearean evaluation\nsystem_prompt = \"\"\"\nYou are an impartial judge that evaluates if text was written by {style}.\n\nAn example piece of text from {style} is:\n{example}\n\nNow, analyze some new text carefully and respond on if it follows the\nsame style of {style}. Be critical to identify any issues in the text.\nThen convert your feedback into a number between 0 and 10: 10 if the text\nis written exactly in the style of {style}, 5 if mixed faithfulness to the\nstyle, or 0 if the text is not at all written in the style of {style}.\n\nThe format of your response should be a JSON dictionary and nothing else:\n{{\"score\": <score between 0 and 10>}}\n\"\"\"\n\n# Define Shakespeare as the style to evaluate\nstyle = \"Shakespeare\"\nexample = \"\"\"To be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\nAnd by opposing end them?\"\"\"\n\n# Format system prompt\nsystem_prompt = system_prompt.format(style=style, example=example)\n\nprint(\"=== System prompt ===\")\nprint(system_prompt)\n\n# Initialize LLM judge (assuming LLMJudgeEvaluator is correctly implemented)\njudge = LLMJudgeEvaluator(llm, system_prompt=system_prompt)\n\n\n\n# Define test samples for Shakespearean evaluation\nbase_samples = [\"To be, or not to be, that is the question.\"]  # Classic Shakespeare\ngenerated_samples = [\n    \"Wherefore dost thou wander, fair traveler?\",\n    \"Hark! Yon moon dost weep ‘pon the silent lake, whilst lovers' sighs do whisper in the night.\",\n    \"Methinks the heavens do roar in anguish dire, for fate’s cruel hand hath struck its mark once more.\",\n    \"Good morrow, gentle sir! What dost thou seek upon this road where shadows dance and speak?\",\n    \"Time, that thief most vile, doth steal our youth, yet leaves behind the echoes of our truth.\"\n] #AI-generated Shakespearean-style text\nstyle_samples = [\"Alas, poor Yorick! I knew him, Horatio.\"]  # Training-set Shakespearean text\n\n\n\n# Compute and print the scores\nbase_scores = compute_scores_in_parallel(base_samples)\nprint(f\"Base: {np.mean(base_scores):.2f} ± {np.std(base_scores):.2f}\")\n\ngenerated_scores = compute_scores_in_parallel(generated_samples)\nprint(f\"Gen: {np.mean(generated_scores):.2f} ± {np.std(generated_scores):.2f}\")\n\nstyle_scores = compute_scores_in_parallel(style_samples)\nprint(f\"Train: {np.mean(style_scores):.2f} ± {np.std(style_scores):.2f}\")\n","metadata":{"id":"2oxxKyfL0Fw-","executionInfo":{"status":"ok","timestamp":1739344098725,"user_tz":-300,"elapsed":5367,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"d6b7c196-d237-49c8-d12d-83e342297b76","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:39.378242Z","iopub.execute_input":"2025-02-18T21:22:39.378598Z","iopub.status.idle":"2025-02-18T21:22:42.398905Z","shell.execute_reply.started":"2025-02-18T21:22:39.378572Z","shell.execute_reply":"2025-02-18T21:22:42.397903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"modern_samples = [\"Hey, are we still on for lunch later?\"]\nmodern_scores = compute_scores_in_parallel(modern_samples)\nprint(f\"Modern: {np.mean(modern_scores):.2f} ± {np.std(modern_scores):.2f}\")\n","metadata":{"id":"rPL_2pTc6BFY","executionInfo":{"status":"ok","timestamp":1739344104337,"user_tz":-300,"elapsed":1783,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"29a4491e-06e1-49bd-de2e-a3fff3975834","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:47.630570Z","iopub.execute_input":"2025-02-18T21:22:47.630914Z","iopub.status.idle":"2025-02-18T21:22:48.754915Z","shell.execute_reply.started":"2025-02-18T21:22:47.630882Z","shell.execute_reply":"2025-02-18T21:22:48.753939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We also tested modern English text, and as expected, it received a score of 0.00. This confirms that our judge is accurately distinguishing between Shakespearean and non-Shakespearean styles.🚀","metadata":{"id":"GSlvv6j7-MBW"}},{"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create a DataFrame for Shakespeare scores\ndf_shakespeare = pd.DataFrame({\n    'Score': [*base_scores, *generated_scores, *style_scores],\n    'Type': ['Base'] * len(base_scores) +\n            ['Generated'] * len(generated_scores) +\n            ['Style'] * len(style_scores)\n})\n\n# Plot with seaborn\nplt.figure(figsize=(8, 5))\nsns.histplot(data=df_shakespeare, x='Score', hue='Type', multiple=\"dodge\", bins=6, shrink=.8)\n\n# Add title and show plot\nplt.title('Distribution of Shakespeare Style Scores')\nplt.xlabel('Score')\nplt.ylabel('Count')\nplt.show()\n","metadata":{"id":"HNBYieyZ7GYo","executionInfo":{"status":"ok","timestamp":1739344211765,"user_tz":-300,"elapsed":610,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"95b4e654-b06d-489a-e224-f48576d450f6","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:22:50.400690Z","iopub.execute_input":"2025-02-18T21:22:50.401026Z","iopub.status.idle":"2025-02-18T21:22:50.657153Z","shell.execute_reply.started":"2025-02-18T21:22:50.400997Z","shell.execute_reply":"2025-02-18T21:22:50.656335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*************\n************\n# Lets proceed to deploy our fine-tuned model\n***********\n**********","metadata":{"id":"_j-nI6oJBCcv"}},{"cell_type":"code","source":"!pip install gradio transformers torch --quiet\n","metadata":{"id":"gOT-HSRD_4Nd","executionInfo":{"status":"ok","timestamp":1739344645466,"user_tz":-300,"elapsed":16472,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"8d1b61e2-276b-4656-8da6-cfc81e15ae90","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:23:14.360534Z","iopub.execute_input":"2025-02-18T21:23:14.360884Z","iopub.status.idle":"2025-02-18T21:23:28.519378Z","shell.execute_reply.started":"2025-02-18T21:23:14.360855Z","shell.execute_reply":"2025-02-18T21:23:28.518221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(chat(\"Tell me a wise lesson, Yoda would.\", only_answer=True))\n","metadata":{"id":"-smQim_3BMrT","executionInfo":{"status":"ok","timestamp":1739345076779,"user_tz":-300,"elapsed":3529,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"abf38169-7980-4f2c-95e3-7b60c7628278","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:23:28.521073Z","iopub.execute_input":"2025-02-18T21:23:28.521373Z","iopub.status.idle":"2025-02-18T21:23:30.684899Z","shell.execute_reply.started":"2025-02-18T21:23:28.521348Z","shell.execute_reply":"2025-02-18T21:23:30.684097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\ndef yoda_chat(user_input):\n    return chat(user_input, only_answer=True)  # Use your fine-tuned model\n\niface = gr.Interface(fn=yoda_chat, inputs=\"text\", outputs=\"text\",\n                     title=\"Chat with Yoda AI\",\n                     description=\"Ask Yoda anything, you can!\")\n\niface.launch()  # Generates a public link\n","metadata":{"id":"DY2tdEvuC5It","executionInfo":{"status":"ok","timestamp":1739345134718,"user_tz":-300,"elapsed":6300,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"817d2164-1c9d-424e-aba9-ee1cf030d936","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:27:54.052888Z","iopub.execute_input":"2025-02-18T21:27:54.053233Z","iopub.status.idle":"2025-02-18T21:27:54.802844Z","shell.execute_reply.started":"2025-02-18T21:27:54.053208Z","shell.execute_reply":"2025-02-18T21:27:54.802151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Image path\nimg_path = \"/kaggle/input/gradio-interface-for-yoda-chat-model/chat.png\"\n\n# Read and show image\nimg = mpimg.imread(img_path)\nplt.imshow(img)\nplt.axis(\"off\")  # Hide axes\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:11:55.748338Z","iopub.execute_input":"2025-02-18T22:11:55.748661Z","iopub.status.idle":"2025-02-18T22:11:55.909634Z","shell.execute_reply.started":"2025-02-18T22:11:55.748634Z","shell.execute_reply":"2025-02-18T22:11:55.908839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## deploying Judge Model","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\ndef judge_yoda_style(text):\n    score = judge.score(text).value  # Get the Yoda-style score\n    return f\"Yoda Style Score: {score:.2f} (Closer to 1.0 = More Yoda-like!)\"\n\niface = gr.Interface(fn=judge_yoda_style, inputs=\"text\", outputs=\"text\",\n                     title=\"Yoda Speech Evaluator\",\n                     description=\"Enter a sentence to see how Yoda-like it is!\")\n\niface.launch()  # Generates a public link\n","metadata":{"id":"c1hWwWvZDGmN","executionInfo":{"status":"ok","timestamp":1739345246352,"user_tz":-300,"elapsed":1533,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"5883c558-fd4a-413f-eae1-72203dbdc8c3","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:28:05.135791Z","iopub.execute_input":"2025-02-18T21:28:05.136083Z","iopub.status.idle":"2025-02-18T21:28:05.852194Z","shell.execute_reply.started":"2025-02-18T21:28:05.136060Z","shell.execute_reply":"2025-02-18T21:28:05.851553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Image path\nimg_path = \"/kaggle/input/gradio-interface-for-yoda-chat-model/yoda evaluator.png\"\n\n# Read and show image\nimg = mpimg.imread(img_path)\nplt.imshow(img)\nplt.axis(\"off\")  # Hide axes\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:10:28.540403Z","iopub.execute_input":"2025-02-18T22:10:28.540728Z","iopub.status.idle":"2025-02-18T22:10:28.678709Z","shell.execute_reply.started":"2025-02-18T22:10:28.540705Z","shell.execute_reply":"2025-02-18T22:10:28.677746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combining CHAT and JUDGE","metadata":{"id":"2T7XPIuXKH5s"}},{"cell_type":"code","source":"import gradio as gr\n\ndef yoda_chat_and_judge(prompt):\n    \"\"\"Generate a Yoda-style response & evaluate it.\"\"\"\n    # Generate a response from your fine-tuned Yoda model\n    response = chat(prompt, only_answer=True)\n\n    # Score the response using the judge\n    score = judge.score(response).value\n\n    # Format the output\n    result = f\"🟢 **Yoda Says:** {response}\\n\\n\" \\\n             f\"🔵 **Yoda Style Score:** {score:.2f} (Closer to 1.0 = More Yoda-like!)\"\n    return result\n\n# Launch Gradio UI\niface = gr.Interface(\n    fn=yoda_chat_and_judge,\n    inputs=\"text\",\n    outputs=\"text\",\n    title=\"🟢 Yoda Chat & Judge\",\n    description=\"Talk to Yoda and get a rating on how Yoda-like his response is!\"\n)\n\niface.launch() \n","metadata":{"id":"Dajwd2DZDjBC","executionInfo":{"status":"ok","timestamp":1739345333064,"user_tz":-300,"elapsed":2067,"user":{"displayName":"Sheema Masood","userId":"06951467210027000021"}},"outputId":"f43ff2bb-68bb-42c0-fdf2-4136e5e23e7c","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:36:30.936314Z","iopub.execute_input":"2025-02-18T21:36:30.936667Z","iopub.status.idle":"2025-02-18T21:36:31.682332Z","shell.execute_reply.started":"2025-02-18T21:36:30.936637Z","shell.execute_reply":"2025-02-18T21:36:31.681489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gradio Interface for Yoda Chat Model 🛠️ ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Image path\nimg_path = \"/kaggle/input/gradio-interface-for-yoda-chat-model/yoda chat and judge.png\"\n\n# Read and show image\nimg = mpimg.imread(img_path)\nplt.imshow(img)\nplt.axis(\"off\")  # Hide axes\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:09:38.058795Z","iopub.execute_input":"2025-02-18T22:09:38.059140Z","iopub.status.idle":"2025-02-18T22:09:38.275391Z","shell.execute_reply.started":"2025-02-18T22:09:38.059113Z","shell.execute_reply":"2025-02-18T22:09:38.274560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***********\n**********","metadata":{"id":"aCMIGLsIKRaQ"}},{"cell_type":"markdown","source":"## 📸 **Interface Overview: Model Evaluation UI**  \n\nTo showcase the working UI, I have included **PNG images** that illustrate the interface in action. These images display the model's evaluation process and results.  \n\n### 🛠 **Features & Functionality**  \n✅ **User-Friendly Interface** – The UI is designed to make evaluating text styles intuitive and efficient.  \n✅ **Real-Time Model Responses** – The interface captures and displays responses generated by the fine-tuned model.  \n✅ **LLM-Based Scoring** – The system scores text similarity to Shakespearean or Yoda-style writing.  \n✅ **Performance Insights** – Users can compare results from the base model, generated text, and the training set.  \n\n### 💡 **Support & Future Improvements**  \n🔹 **Suggestions Welcome!** – If you have any ideas for enhancing the evaluation system, feel free to share.  \n🔹 **Enhanced UI Coming Soon!** – Future updates may include an improved design and additional interactive features.  \n\n🚀 *Hope this provides a clear view of the deployed model in action!*  \n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}